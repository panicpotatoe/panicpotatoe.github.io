{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Pre-processing\n",
    "## 1.1. Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = \"/Users/nhantran/Library/CloudStorage/GoogleDrive-little.tea.07@gmail.com/My Drive/Workspace/VinBigData/gsm/customer-insights\"\n",
    "DATA_PATH = \"/Users/nhantran/Library/CloudStorage/GoogleDrive-little.tea.07@gmail.com/My Drive/Workspace/VinBigData/gsm/customer-insights\"\n",
    "\n",
    "SAMPLE_BOOKING_GSM = \"data_booking-gsm_part_0.csv\"\n",
    "SAMPLE_BOOKING_PARTNER = \"data_booking-partner_part_0.csv\"\n",
    "\n",
    "DFF_CHUNK_SIZE = 25e6  # 25MB per chunk\n",
    "EXTRACTED_DATA_PATH = \"./extracted-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Import libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration\n",
    "## 2.1. Read the data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the CSV file into a Dask DataFrame\n",
    "ddf = dd.read_csv(\n",
    "    f\"{DATA_PATH}/{SAMPLE_BOOKING_GSM}\",\n",
    "    blocksize=DFF_CHUNK_SIZE,\n",
    "    dtype={\n",
    "        \"business_note\": \"str\",\n",
    "        \"message_error\": \"str\",\n",
    "        \"note\": \"str\",\n",
    "        \"other_reason\": \"str\",\n",
    "        \"promotion_code\": \"str\",\n",
    "        \"promotion_session_id\": \"str\",\n",
    "    },\n",
    ")\n",
    "ddf.compute()\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data cleaning\n",
    "### 2.2.1. Target on `start_address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'address' column into parts using the comma as the delimiter\n",
    "start_address_parts = ddf[\"start_address\"].str.split(\",\")\n",
    "\n",
    "# Define the column names for the administrative levels\n",
    "columns = [\n",
    "    \"start_address_admin_lvl_4\",\n",
    "    \"start_address_admin_lvl_3\",\n",
    "    \"start_address_admin_lvl_2\",\n",
    "    \"start_address_admin_lvl_1\",\n",
    "    \"start_address_admin_lvl_0\",\n",
    "]\n",
    "\n",
    "# Create a Dask DataFrame with computed columns\n",
    "ddf_start_address = dd.concat(\n",
    "    [start_address_parts.str[i].str.strip() for i in range(len(columns))], axis=1\n",
    ")\n",
    "ddf_start_address.columns = columns\n",
    "\n",
    "# Compute\n",
    "ddf_start_address.compute()\n",
    "\n",
    "# Print the top 5 rows of dff\n",
    "print(ddf_start_address.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'address' column into parts using the comma as the delimiter\n",
    "end_address_parts = ddf[\"end_address\"].str.split(\",\")\n",
    "\n",
    "# Define the column names for the administrative levels\n",
    "columns = [\n",
    "    \"end_address_admin_lvl_4\",\n",
    "    \"end_address_admin_lvl_3\",\n",
    "    \"end_address_admin_lvl_2\",\n",
    "    \"end_address_admin_lvl_1\",\n",
    "    \"end_address_admin_lvl_0\",\n",
    "]\n",
    "\n",
    "# Create a Dask DataFrame with computed columns\n",
    "ddf_end_address = dd.concat(\n",
    "    [end_address_parts.str[i].str.strip() for i in range(len(columns))], axis=1\n",
    ")\n",
    "ddf_end_address.columns = columns\n",
    "\n",
    "# Compute\n",
    "ddf_end_address.compute()\n",
    "\n",
    "# Print the top 5 rows of dff\n",
    "print(ddf_end_address.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge `ddf_start_address` and `ddf_end_address` to the original dff\n",
    "ddf_final = dd.concat([ddf, ddf_start_address, ddf_end_address], axis=1)\n",
    "ddf_final.compute()\n",
    "ddf_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the number of rows and columns\n",
    "print(f\"rows: {ddf_final.shape[0].compute()}\")\n",
    "print(f\"columns: {ddf_final.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all unique values in the `start_address_admin_lvl_0` column\n",
    "# print(f\"Unique values in start_address_admin_lvl_0: {ddf_final['start_address_admin_lvl_0'].unique().compute()}\")\n",
    "print(f\"{ddf_final['start_address_admin_lvl_0'].compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "street: Einstein,\n",
      "ward: London,\n",
      "district: United,\n",
      "province: Kingdom\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "activated = spacy.prefer_gpu()\n",
    "EXTRACTED_DATA_PATH = \"./extracted-data\"\n",
    "\n",
    "# Load your CSV files containing street names, ward names, district names, and province names\n",
    "street_df = pd.read_csv(f\"{EXTRACTED_DATA_PATH}/streets.csv\")\n",
    "# street_df.head(2)\n",
    "ward_df = pd.read_csv(f\"{EXTRACTED_DATA_PATH}/wards.csv\")\n",
    "district_df = pd.read_csv(f\"{EXTRACTED_DATA_PATH}/districts.csv\")\n",
    "province_df = pd.read_csv(f\"{EXTRACTED_DATA_PATH}/provinces.csv\")\n",
    "\n",
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def parse_address(address):\n",
    "    doc = nlp(address)\n",
    "    street = \"\"\n",
    "    ward = \"\"\n",
    "    district = \"\"\n",
    "    province = \"\"\n",
    "    for token in doc:\n",
    "        # Check if the token is a recognized administrative level\n",
    "        if token.ent_type_ == \"GPE\":\n",
    "            if not street:\n",
    "                street = token.text\n",
    "            elif not ward:\n",
    "                ward = token.text\n",
    "            elif not district:\n",
    "                district = token.text\n",
    "            elif not province:\n",
    "                province = token.text\n",
    "        else:\n",
    "            # If the token is not a recognized entity, check CSV files for matches\n",
    "            if not street:\n",
    "                street = get_street_from_csv(token.text)\n",
    "            if not ward:\n",
    "                ward = get_ward_from_csv(token.text)\n",
    "            if not district:\n",
    "                district = get_district_from_csv(token.text)\n",
    "            if not province:\n",
    "                province = get_province_from_csv(token.text)\n",
    "    return street, ward, district, province\n",
    "\n",
    "\n",
    "def get_street_from_csv(token):\n",
    "    for street_name in street_df[\"streets\"]:\n",
    "        if str(token).lower() in str(street_name).lower():\n",
    "            return street_name\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_ward_from_csv(token):\n",
    "    for ward_name in ward_df[\"full_name\"]:\n",
    "        if token.lower() in ward_name.lower():\n",
    "            return ward_name\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_district_from_csv(token):\n",
    "    for district_name in district_df[\"full_name\"]:\n",
    "        if token.lower() in district_name.lower():\n",
    "            return district_name\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_province_from_csv(token):\n",
    "    for province_name in province_df[\"full_name\"]:\n",
    "        if token.lower() in province_name.lower():\n",
    "            return province_name\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def standardize_address(address):\n",
    "    street, ward, district, province = parse_address(address)\n",
    "    standardized_address = (\n",
    "        f\"street: {street},\\nward: {ward},\\ndistrict: {district},\\nprovince: {province}\"\n",
    "    )\n",
    "    return standardized_address\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_address = \"215 Minh Khai, Vinh Tuy, Hai Ba Trung, Ha Noi\"\n",
    "standardized = standardize_address(input_address)\n",
    "print(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "street: 215,\n",
      "ward: Phường Minh Khai,\n",
      "district: Huyện Yên Minh,\n",
      "province: Thành phố Hồ Chí Minh\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# input_address = \"215 Minh Khai, Vinh Tuy, Hai Ba Trung, Ha Noi\"\n",
    "input_address = \"215 Minh Khai, Vĩnh Tuy, Hai Bà Trưng, Hà Nội\"\n",
    "standardized = standardize_address(input_address)\n",
    "print(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
